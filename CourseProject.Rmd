---
title: "Practical Machine Learning Project"
author: "LDBT"
date: "Friday, May 22, 2015"
output: html_document
---

### Background

This project uses data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who are asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of the project is to predict the manner in which they did the exercise. 

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

- Exactly according to the specification:  Class A
- Throwing the elbows to the front:  Class B
- Lifting the dumbbell only halfway:  Class C
- Lowering the dumbbell only halfway:  Class D
- Throwing the hips to the front:  Class E

More information is available from the website here: 
http://groupware.les.inf.puc-rio.br/har 
(see the section on the Weight Lifting Exercise Dataset).  

#### Data 

The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data is available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

#### Project objectives

1. Predict the manner in which they did the exercise. This is the "classe" variable in the training set. Any of the other variables can be used to predict with. 
2. Create a report describing 
    - how the model was built
    - how it used cross validation
    - what is the expected out of sample error
    - how the choices of model were made
3. The prediction model will be used to predict 20 different test cases. 
4. The submission  consist of a link to a Github repo with R markdown and compiled HTML file describing the analysis (< 2000 words, number of figures < 5). A gh-pages branch such that  the HTML page can be viewed online is submitted to assist graders.  
5. The machine learning algorithm will be applied to the 20 test cases available in the test data above. 

#### Reproducibility 

Due to security concerns with the exchange of R code, the code will not be run during the evaluation.  View the compiled HTML version of the analysis. 

### Program environment
####Libraries used
```{r libraries, echo=TRUE, warning=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)

#### Randon number seed
```{r randomseed}
set.seed(12345)
```

### Data
#### Download training a test sets
```{r data, cache=FALSE}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# Convert entries of "NA","#DIV/0!","" to "NA"
training.dta <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing.dta <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
# Delete columns that have all zeros
training<-training.dta[,colSums(is.na(training.dta)) == 0]
testing <-testing.dta[,colSums(is.na(testing.dta)) == 0]
# Delete columns with all missing values
training<-training[,colSums(is.na(training)) != 'NA']
testing <-testing[,colSums(is.na(testing)) != 'NA']
#Delete the first seven columns as they have no relevant data to predicct 'classe'
training   <-training[,-c(1:7)]
testing <-testing[,-c(1:7)]
```
#### Check the results
```{r sets}
dim(training)   # rows/cols training set
dim(testing)    # rows/cols testing set
```

#### Partition the training set     
- Training data set contains `r dim(training)[1]` rows and `r dim(training)[2]` columns and is divided into two subsets, 60% for training, 40% for testing using variable `classe`.  The original testing set will be used only once to test the completed model.

```{r partition}
#Create vector with rows selected for training set.
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
# Create training and testing subsets with above vector
ssTraining <- training[inTrain, ]; ssTesting <- training[-inTrain, ]
```
#### Check the subsets
```{r}
dim(ssTraining); dim(ssTesting)

```
The ratio of observations in the training subset to the total number of observatiosn is **`r round(dim(ssTraining)[1]/(dim(ssTraining)[1]+dim(ssTesting)[1]),3)`** which is the proportion desired.

#### Display the data
Variable “classe” contains 5 levels: A, B, C, D and E. We compare te distributions of the `classe` variable in the training and testing subsets.

```{r dispdata }
par(mfrow=c(1,2))
plot(ssTraining$classe, col="blue", main="Training subset", xlab="classe Levels", ylab="Frequency")
plot(ssTesting$classe, col="red", main="Testing subset", xlab="classe Levels", ylab="Frequency")
```

The plots have the same shape indicating that the distribution of the `classe` variable in the training and testing subsets is similar.

### Prediction
#### First model using Decision Tree 

```{r train.model1, cache=TRUE}
model1 <- rpart(classe ~ ., data=ssTraining, method="class") # model generation
prediction1 <- predict(model1, ssTesting, type = "class") # # Predicting 1

```

```{r plot.mod1, cache=TRUE}
# Tree plots
par(mfrow=c(1,1))
rpart.plot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)

```

Test the model 1 on the testing subset  

```{rtest.mod1}

confusionMatrix(prediction1, ssTesting$classe)

```
#### Second model using Random Forest

```{r train.model2, cache=TRUE}
model2 <- randomForest(classe ~. , data=ssTraining, method="class")
prediction2 <- predict(model2, ssTesting, type = "class")
```

Test model 2 on the testing subset  

```{r test.model2}
# Test results on subTesting data set:
cmPred2 <- confusionMatrix(prediction2, ssTesting$classe)
cmPred2
```
### Report

#### Error Estimation with Cross validation

Cross-validation was performed by dividing the training data in two subsets:  the training subset `ssTraining` had 60% of the original data set.  The validation subset `ssTesting` had 40% of the original training data set.  The selection was done randomly without replacement. The training set was used to develop the model with two different algorithms.   

#### Expected out-of-sample error

The expected out-of-sample error is the quantity `1-accuracy` when it is applied to the validation data. The validation data is a subset of the training data which in the present report is 40% of the original sample.  
- Expected out of sample error:  `round(1 - cmPred2$overall["Accuracy"],4)`

#### Model selection

The methods used for model selection were `Decision Tree` and `Random Forest`.  The `Random Forest` method provided gretater accuracy and was therefore chosen for the final validation in the original testing set. 

### Model validation and Submission

#### Final Model
```{r}
# predict outcome levels on the original Testing data set using Random Forest algorithm
predictFinal <- predict(model2, testing, type = "class")
predictFinal
```

#### Files for test submission

```{r}
# Write files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(predictFinal)

```
